{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86646ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESERCIZIO1: ESTRARRE LA TABELLA dimproduct dal database AdventureWorks\n",
    "''' sulla colonna DealerPrice, utilizzando il metodo .round(), arrotondare i valori alle due cifre decimali, e poi al valore intero più vicino\n",
    "    utilizzando il metodo .clip(), fare in modo che i valori siano compresi tra un minimo di 0 e un massimo di 1000'''\n",
    "\n",
    "import os\n",
    "import dotenv \n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "\n",
    "dotenv.load_dotenv(dotenv_path=\"adv_sql.env\", override=True,) #carica il file env\n",
    "\n",
    "username = os.getenv(\"username\") #credenziali\n",
    "password = os.getenv(\"password\")\n",
    "host = os.getenv(\"host\")\n",
    "dbname = os.getenv(\"dbname\")\n",
    "\n",
    "connection_string = f\"mysql+pymysql://{username}:{password}@{host}/{dbname}\" #stringa di connessione\n",
    "db_engine = sqlalchemy.create_engine(connection_string) #connessione al db\n",
    "\n",
    "query = \"SELECT * FROM dimproduct\"\n",
    "df = pd.read_sql(query, db_engine)\n",
    "\n",
    "df[\"DealerPrice\"] = df[\"DealerPrice\"].round(2) #arrotondo alle due cifre decimali\n",
    "df[\"DealerPrice\"] = df[\"DealerPrice\"].round(0) #arrotondo all'intero più vicino\n",
    "df[\"DealerPrice\"] = df[\"DealerPrice\"].clip(lower=0, upper=1000) #setto i valori tra 0 e 1000\n",
    "\n",
    "df[\"DealerPrice\"].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ab085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESERCIZIO2: creare un DataFrame sintetico, che contiene i guadagni mensili di diverse annate\n",
    "''' calcolare la somma cumulativa dei guadagni utilizzando il metodo .cumsum()\n",
    "    come sopra, ma raggruppato per ogni anno usando prima un .groupby() '''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "years = 5 \n",
    "guadagni = pd.DataFrame({\"Mese\": list(\"GFMAMGLASOND\"*years), \n",
    "                         \"Anno\": np.repeat(list(range(years)), 12), \n",
    "                         \"Valore\": np.random.randint(800, 5000, 12*years)})\n",
    "\n",
    "#creo colonna vendite cumulative e inserisco la cumsum\n",
    "guadagni.loc[:, \"Vendite Cumulaive\"] = guadagni.loc[:, \"Valore\"].cumsum() \n",
    "\n",
    "anni = guadagni.groupby(\"Anno\")[\"Valore\"].cumsum()\n",
    "anni\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e7778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESERCIZIO3: ESTRARRE LA TABELLA dimcustomer dal database AdventureWorks\n",
    "''' trasformare  i nomi dei clienti in modo che abbiano solo lettere minuscole, e i cognomi in modo che abbiano solo lettere maiuscole\n",
    "    sulla colonna EmailAddress, utilizzando il metodo .str.split(), estrarre nome utente e dominio\n",
    "    sulla colonna Phone, estrarre ogni parte del numero (ad es. da \"1 (11) 500 555-0162\" a [\"1\", \"(11)\", \"500\", \"555-0162\"])\n",
    "    utilizzando il metodo .str.contains(), estrarre tutti gli indirizzi e-mail che contengono il numero \"21\" '''\n",
    "\n",
    "import os\n",
    "import dotenv \n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "\n",
    "dotenv.load_dotenv(dotenv_path=\"adv_sql.env\", override=True,) #carica il file env\n",
    "\n",
    "username = os.getenv(\"username\") #credenziali\n",
    "password = os.getenv(\"password\")\n",
    "host = os.getenv(\"host\")\n",
    "dbname = os.getenv(\"dbname\")\n",
    "\n",
    "connection_string = f\"mysql+pymysql://{username}:{password}@{host}/{dbname}\" #stringa di connessione\n",
    "db_engine = sqlalchemy.create_engine(connection_string) #connessione al db\n",
    "\n",
    "query = \"SELECT * FROM dimcustomer\"\n",
    "df = pd.read_sql(query, db_engine)\n",
    "\n",
    "df[\"FirstName\"] = df[\"FirstName\"].str.lower() #nomi minuscoli\n",
    "df[\"LastName\"] = df[\"LastName\"].str.upper() #cognomi maiuscoli\n",
    "\n",
    "#df[\"EmailAddress\"] = df[\"EmailAddress\"].str.split(pat=\"@\") #divido utente e dominio separati da @\n",
    "\n",
    "df[\"Phone\"] = df[\"Phone\"].str.split() #divido le cifre separate da whitespace\n",
    "\n",
    "num_21 = df.loc[df[\"EmailAddress\"].str.contains(\"21\", regex=False)]  #cerco le email con 21 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d197f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESERCIZIO3: ESTRARRE LA TABELLA dimcustomer dal database AdventureWorks\n",
    "''' estrarre tutti gli indirizzi e-mail che contengono il numero \"20\" oppure il numero \"10\" \n",
    "    calcolare la lunghezza di ogni indirizzo e-mail ed estrarre i cinque più lunghi e i cinque più corti \n",
    "    modificare il dominio degli indirizzi e-mail da \"adventure-works.com\" a \"aw-db.com\" mediante il metodo .str.replace() \n",
    "    dalla colonna AddressLine1 estrarre tutti gli indirizzi che contengono la sottostringa \"Street\" '''\n",
    "\n",
    "import os\n",
    "import dotenv \n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "\n",
    "dotenv.load_dotenv(dotenv_path=\"adv_sql.env\", override=True,) #carica il file env\n",
    "\n",
    "username = os.getenv(\"username\") #credenziali\n",
    "password = os.getenv(\"password\")\n",
    "host = os.getenv(\"host\")\n",
    "dbname = os.getenv(\"dbname\")\n",
    "\n",
    "connection_string = f\"mysql+pymysql://{username}:{password}@{host}/{dbname}\" #stringa di connessione\n",
    "db_engine = sqlalchemy.create_engine(connection_string) #connessione al db\n",
    "\n",
    "query = \"SELECT * FROM dimcustomer\"\n",
    "df = pd.read_sql(query, db_engine)\n",
    "\n",
    "num_20 = df[\"EmailAddress\"].str.contains((\"20\"), regex=False) #email con 20\n",
    "num_10 = df[\"EmailAddress\"].str.contains((\"10\"), regex=False) #email con 10\n",
    "num_20_10 = df.loc[num_20 | num_10] #filtro email\n",
    "\n",
    "emails = df[\"EmailAddress\"]\n",
    "lista = []\n",
    "\n",
    "#per ogni email conto la lunghezza della email, si inserisce tutto in una lista e stampo i primi 5 e ultimi 5\n",
    "for email in emails: \n",
    "    lung = len(email)\n",
    "    lista.append(lung)\n",
    "\n",
    "lista.sort(reverse=False)\n",
    "#print(lista[0:5]) #primi 5\n",
    "#print(lista[-5:]) #ultimi 5\n",
    "\n",
    "#con .str.len()\n",
    "emails = df[\"EmailAddress\"]\n",
    "lunghezze = emails.str.len()\n",
    "sort_lung = lunghezze.sort_values()\n",
    "primi_5 = sort_lung.head(5)\n",
    "ultimi_5 = sort_lung.tail(5)\n",
    "\n",
    "#print(primi_5, ultimi_5)\n",
    "\n",
    "df[\"EmailAddress\"] = df[\"EmailAddress\"].str.replace(\"adventure-works.com\", \"aw-db.con\") #sostituisco dominio\n",
    "\n",
    "street = df.loc[df[\"AddressLine1\"].str.contains(\"Street\", regex=False)]\n",
    "street\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31775e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESERCIZIO4: CARICARE IL DATASET facebook.csv che contiene dei post con data di pubblicazione, tipo (foto, video, …) e numero di reactions raccolte:\n",
    "''' con la funzione pd.to_datetime() convertire la colonna status_published in formato Timestamp\n",
    "    utilizzando gli attributi .dt.year , .dt.month , .dt.day , .dt.dayofweek , .dt.dayofyear, \n",
    "    ottenere informazioni specifiche sulle date delle transazioni, come l'anno, il mese, il giorno della settimana, il giorno dell'anno\n",
    "    estrarre solo i post relativi al 2012\n",
    "    estrarre solo i post relativi a maggio 2018\n",
    "    confrontare il numero di post pubblicati nei weekend rispetto al numero di post pubblicati nel resto della settimana\n",
    "    trovare il primo e ultimo post pubblicati in ogni anno\n",
    "    quanti tipi di post ci sono? e quanti per ogni tipo? '''\n",
    "\n",
    "import pandas as pd\n",
    "url = \"Dataset/facebook.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df[\"status_published\"] = pd.to_datetime(df.loc[:, \"status_published\"]) #trasformo in timestamp\n",
    "\n",
    "#anno, mese, giorno, giorno della settimana e giorno dell'anno\n",
    "df[\"year\"] = df.loc[:, \"status_published\"].dt.year\n",
    "df[\"month\"] = df.loc[:, \"status_published\"].dt.month\n",
    "df[\"day\"] = df.loc[:, \"status_published\"].dt.day\n",
    "df[\"weekday\"] = df.loc[:, \"status_published\"].dt.dayofweek\n",
    "df[\"yearday\"] = df.loc[:, \"status_published\"].dt.dayofyear\n",
    "\n",
    "filtro_2012 = df.loc[:, \"year\"] == 2012 #filtro che prende i post dell'anno 2012\n",
    "post_2012 = df.loc[filtro_2012]\n",
    "\n",
    "filtro_05_2018 = (df.loc[:, \"year\"] == 2018) & (df.loc[:, \"month\"]== 5 )\n",
    "post_05_2018 = df.loc[filtro_05_2018]\n",
    "\n",
    "num_post = df.groupby(\"weekday\").size() #numero di post pubblicati nei giorni della settimana\n",
    "\n",
    "first = df.groupby(\"year\")[\"status_type\"].first() #primo post per ogni anno\n",
    "last = df.groupby(\"year\")[\"status_type\"].last() #ultimo post per ogni anno\n",
    "\n",
    "df.groupby(\"status_type\").size() #tipi di post e quanti per ogni tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cce52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESERCIZIO5: CARICARE IL DATASET pokemon.csv \n",
    "''' Tramite i metodi .isnull() e .sum() controlliamo se ci sono valori nulli nel dataset e contiamo quanti valori nulli ci sono in ogni colonna \n",
    "    ci sono valori nulli? se sì, avrebbe senso cercare di riempirli? \n",
    "    eliminare le righe che contengono valori nulli '''\n",
    "\n",
    "import pandas as pd\n",
    "url = \"Dataset/pokemon.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df.isnull().sum() #valori nulli, avrebbe senso riempirli solo per mantenere tutte le righe\n",
    "df.dropna() #elimino righe che hanno valori null, così si perdono dati però"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c05e7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESERCIZIO6: CARICARE IL DATASET automobile.csv\n",
    "''' ci sono valori nulli? dove? quanti?\n",
    "    quali righe hanno un valore nullo nella colonna num-of-doors?\n",
    "    esaminando i dati nel dataset, cercare una logica per sostituire i valori nulli nella colonna num-of-doors '''\n",
    "\n",
    "import pandas as pd\n",
    "url = \"Dataset/automobile.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df.isnull().sum()  #37 nulli in normalized-losses e 2 in num-of-doors\n",
    "df[\"num-of-doors\"] = df[\"num-of-doors\"].fillna(2) #riempio con il minimo di porte \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dddfe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['automobile.csv', 'elections.csv', 'france.csv', 'hepatitis.csv', 'house.csv', 'income.csv', 'mice.csv', 'nba.csv', 'pokemon.csv', 'population.csv', 'seeds.csv', 'traffic.csv', 'wikipedia.csv']\n"
     ]
    }
   ],
   "source": [
    "#ESERCIZIO7: dato il seguente dataframe che raccoglie le misurazioni di un sensore che misura la temperatura atmosferica giornaliera\n",
    "''' il sensore a volte non funziona, dunque alcuni dati sono mancanti: quale sarebbe la migliore strategia per gestirli? \n",
    "    nella directory dei beginner_datasets, quali sono i dataset che contengono dati nulli? (vedere slide)\n",
    "    usare un ciclo for per esaminare tutti i nomi dei file\n",
    "    selezionare solo i nomi di file con estensione .csv (costrutto if)\n",
    "    leggere di volta in volta il file in esame, e caricarlo in un DataFrame con la funzione .read_csv()\n",
    "    utilizzare il metodo .isna() per trovare la maschera booleana dei dati nulli\n",
    "    contare i dati nulli, utilizzando .sum(); potremmo doverlo utilizzare più di una volta\n",
    "    stampare, o memorizzare in una list, solo i nomi dei file che contengono dati nulli'''\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "temp = pd.DataFrame({\"Giorno\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], \n",
    "                     \"Temperature\": [18, 19, 18, np.nan, 21, 20, 20, np.nan, 21, 23, np.nan, 23, 24]})\n",
    "\n",
    "#dato che il sensore a volte non funziona, sarebbe meglio un'interpolazione \n",
    "temp.interpolate()\n",
    "\n",
    "datasets = os.listdir(\"Datasets_EPICODE/beginner_datasets\") #lista nomi in directory\n",
    "lista = [] #inizializzazione per la lista finale\n",
    "for dataset in datasets: \n",
    "    if dataset[-4:] == \".csv\": #prendo i dataset che terminano con .csv\n",
    "        df = pd.read_csv(f\"Datasets_EPICODE/beginner_datasets/{dataset}\") #leggo i file csv\n",
    "        nulli = df.isna().sum().sum() #calcolo nulli nel df \n",
    "        if nulli > 1: #se sono presenti nulli, si aggiunge il df nella lista\n",
    "            lista.append(dataset)\n",
    "print(lista)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epicode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
